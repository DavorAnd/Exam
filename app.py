# coding: utf-8

"""Another copy of Exam Collab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13tCo7PcaOC8WS_qVoPPlSOSGES8GkEaK
"""

# Bank Marketing
# We wanna do first make the bank_marketing.csv a functional
# 1. Import the libraries and Data 	EDA
# 2. Clean the dataset
# 3. Visualization- plots --look for imbalance for in data
# 2. Clean the dataset
# 4. Scale the data for UML
# 4. Clustering for UML
# 4. Decision on model ... Classification vs Regression model .... for SML
# 5. Feature Engineering ... what can we add to the dataset to make it more resilient ... calculate stuff or normalize stuff
# 6. Test the models (take 3 models and test them out to see what result we get
# 7. Evaluate the model--- feature importance
# 8. App or Interface

#installing necessary libraries

# !pip install xgboost -U -q #library for supervised machine learning (reduction and classification)
# !pip install sklearn -U -q #library for machine learning and data analysis
# !pip install gradio -U -q #library to create interactive web applications
# !pip install umap-learn -U -q #library for dimensionality reduction
# !pip install streamlit

import numpy as np #is primarily used for numerical and array-based operations
import seaborn as sns #is a data visualization library that makes it easy to create attractive and informative statistical graphics
import statsmodels.api as sm #used for statistical modeling and hypothesis testing. Helps you to analyze data and build statistical models to understand relationships between variables
import matplotlib.pyplot as plt #it allows you to make various types of graphs to help you visualize and understand your data
from xgboost import XGBRegressor #provides an efficient and powerful algorithm to handle large datasets and perform classification or regression tasks
from sklearn.metrics import mean_squared_error #it gives you tools for evaluating the performance of machine learning models. It helps you measure how well your model is doing its job
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder # helps you prepare your data for machine learning. It's like a toolbox for transforming and scaling your data so that it works better with machine learning algorithms.
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import streamlit as st


df = pd.read_csv('https://raw.githubusercontent.com/DavorAnd/BDS-Phyton/main/bank_marketing.csv')

"""**EDA SECTION**"""

df.info()

df.isna().sum()

df.columns #Cheking the columns

df.head() # looking at first 5 rows of data

df.poutcome.unique()

df.describe() #looking into different statistical values in regards to specific columns

df.shape # looking at how the data is shaped

df.isnull().sum() # cheking for the empty or null values and we see that there are none

"""**Visualization and filtering **"""

df.columns

# merge the loans into one column and dropping the housing and loan
df['Loans'] = df.apply(lambda row: any(row== "yes"), axis=1)
df = df.drop(['housing','loan'], axis=1)

numerical = ['age']
categorical = ['job', 'education']

#Showing the numerical variables in histoplots
for column in numerical :
    plt.hist(df[column], bins= 30)
    plt.title(f'Distibution of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.show()

sns.boxplot(data=df, x="campaign")  # outliers are everything abouve 8

"""Filtering the data"""

df = df[df.ne('unknown').all(axis =1)] # dropping all the unknown values in the dataframe
df = df[df.campaign <= 8]
sns.boxplot(data=df, x="campaign")

mapping = {'no':False,'yes':True}
df.replace({"y"  :mapping}, inplace=True)

df.head()
df.education = df.education.str.replace('.',' ')

clean_df = df.drop(columns=['marital','default','contact','month','day_of_week','duration','campaign','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed','poutcome'], axis=1)

clean_df

clean_df.education.unique()

"""Scaling  and Encoding  data"""

clean_df

nums = ['age',	'y',	'Loans']
cats = ['job',	'education']
label_encoders = {}
encoded_categorical = pd.DataFrame()

for cat_col in categorical:
    label_encoder = LabelEncoder()
    clean_df[cat_col + '_encoded'] = label_encoder.fit_transform(clean_df[cat_col])
    label_encoders[cat_col] = label_encoder

# Combine the encoded categorical columns into one DataFrame
encoded_categorical = clean_df[[col + '_encoded' for col in categorical]]
X = pd.concat([clean_df[nums], encoded_categorical], axis=1)

encoded_categorical

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA()
X_pca = pca.fit_transform(X_scaled)

pca = PCA(n_components=2)
X_pca_2d = pca.fit_transform(X_scaled)

# Determining the optimal number of clusters for KMeans using the Elbow method
clusters = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(X_pca_2d)
    clusters.append(kmeans.inertia_)
plt.figure(figsize=(8,4))
plt.plot(range(1, 11), clusters, marker='o', linestyle='--')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.title('KMeans Elbow Method for Optimal k')
plt.show()

# Conducting KMeans clustering (assuming 3 clusters from the elbow method)
kmeans = KMeans(n_clusters=3, random_state=0).fit(X_pca_2d)
df['cluster'] = kmeans.fit_predict(X_pca_2d)
labels = kmeans.labels_

plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=clean_df['cluster'], cmap='viridis')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.title('Cluster Visualization')
plt.show()

"""# Supervised part"""

df

scaler = StandardScaler()
SML_X_scaled = scaler.fit_transform(X)
pca = PCA()
SML_X_pca = pca.fit_transform(SML_X_scaled)

numerical_cols = ['age', 'y', 'Loans']
categorical_cols = ['job', 'education']
assert all(col in clean_df.columns for col in numerical_cols + categorical_cols), "One or more columns not found in clean_df"

SML_X = df.drop('cluster', axis=1)
SML_y = df['cluster']
missing_cols = [col for col in numerical_cols + categorical_cols if col not in clean_df.columns]
if missing_cols:
  print("Missing columns:", missing_cols)

SML_X_train, SML_X_test, SML_y_train, SML_y_test = train_test_split(SML_X, SML_y, test_size=0.2, random_state=42)

# Define preprocessing for numerical features
numerical_transformer = StandardScaler()

# Define preprocessing for categorical features
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Combine transformers in a ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

SML_X

# Logistic Regression pipeline
lr_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', LogisticRegression(random_state=42, max_iter=1000))
])

# Fit the model using the pipeline
lr_pipeline.fit(SML_X_train, SML_y_train)

# Make predictions
lr_y_pred = lr_pipeline.predict(SML_X_test)

# Evaluate the model
lr_accuracy = accuracy_score(SML_y_test, lr_y_pred)
print(f'Logistic Regression Accuracy: {lr_accuracy:.2f}')

# Decision Tree pipeline
dt_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', DecisionTreeClassifier(random_state=42))
])

dt_pipeline.fit(SML_X_train, SML_y_train)
dt_y_pred = dt_pipeline.predict(SML_X_test)
dt_accuracy = accuracy_score(SML_y_test, dt_y_pred)
print(f'Decision Tree Accuracy: {dt_accuracy:.2f}')

# Random Forest pipeline
rf_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', RandomForestClassifier(random_state=42))
])

rf_pipeline.fit(SML_X_train, SML_y_train)
rf_y_pred = rf_pipeline.predict(SML_X_test)
rf_accuracy = accuracy_score(SML_y_test, rf_y_pred)
print(f'Random Forest Accuracy: {rf_accuracy:.2f}')

model_accuracies = pd.DataFrame({
    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest'],
    'Accuracy': [lr_accuracy, dt_accuracy, rf_accuracy]
})

print(model_accuracies)

def predict_cluster(age, job, education, loans):
    user_data = pd.DataFrame({'age': [age], 'job': [job], 'education': [education], 'Loans': [loans]})
    cluster = lr_pipeline.predict(user_data)
    if cluster[0] == 0:
        return "good"
    elif cluster[0] == 1:
        return "hello"
    else:
        return "bye"

# Streamlit UI
st.title('Cluster Prediction')

# Input values
age = st.number_input('Age', value=30)
job = st.selectbox('Job', clean_df['job'].unique())
education = st.selectbox('Education', clean_df['education'].unique())
loans = st.checkbox('Loans (Yes/No)')

# Predict and display cluster
if st.button('Predict Cluster'):
    result = predict_cluster(age, job, education, loans)
    st.write('Message based on Cluster:', result)